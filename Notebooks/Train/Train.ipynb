{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Anomaly Detection Model Training for Acoustic Data\n",
    "#\n",
    "This notebook trains and evaluates three different models for anomaly detection using your audio dataset:\n",
    "#\n",
    "- **K-means Clustering:** An unsupervised method trained on normal data only. Anomaly scores are computed as the distance to the learned cluster center.\n",
    "- **Convolutional Autoencoder:** An unsupervised deep learning approach where a higher reconstruction error indicates an anomaly.\n",
    "- **Support Vector Machine (SVM):** A supervised classifier trained on both normal and anomalous features.\n",
    "- **Gaussian Mixture Model (GMM):** Fits a probabilistic model to normal data. Test samples\n",
    "   with very low likelihood (log-probability) are flagged as anomalies.\n",
    "- **LSTM Autoencoder:** Groups consecutive sliding-window feature vectors into sequences. An LSTM\n",
    "   autoencoder is trained on normal sequences; high reconstruction error on a test sequence suggests an anomaly.\n",
    "- **GAN-based Anomaly Detection:** A GAN is trained on normal data. After training, the discriminator’s\n",
    "   output (interpreted as the probability that a sample is normal) is used to compute an anomaly score.\n",
    "#\n",
    "Evaluation metrics include accuracy, precision, recall, F1 score, and ROC curves.\n",
    "#\n",
    "**Dataset:** We use two recordings—one \"normal\" and one \"anomalous\"—and segment each into overlapping windows to extract features.\n"
   ],
   "id": "8786890a33cf45d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Imports and Setup\n",
   "id": "26d29591a8e20a07"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             confusion_matrix, roc_auc_score, roc_curve)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from scipy.fft import fft\n",
    "from matplotlib import ticker\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "\n"
   ],
   "id": "c75dfde2f90e07f7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Data Loading and Feature Extraction\n",
    "#\n",
    "We load the \"normal\" and \"anomalous\" recordings and then segment each audio signal into overlapping windows.\n",
    "For each window, we extract features including:\n",
    "- 13 MFCCs (mean values)\n",
    "- Spectral features: centroid, rolloff, contrast\n",
    "- Temporal feature: zero crossing rate\n",
    "#\n",
    "These features form the basis for our models.\n"
   ],
   "id": "9d3b8d2a87f4da23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_audio_file(file_path):\n",
    "    \"\"\"Load a .wav file and return audio and sample rate.\"\"\"\n",
    "    audio, sr = librosa.load(file_path, sr=None)\n",
    "    return audio, sr\n",
    "\n",
    "\n",
    "def extract_features(audio, sr, n_mfcc=13):\n",
    "    \"\"\"Extract MFCC, spectral, and temporal features from an audio segment.\"\"\"\n",
    "    # MFCCs (mean for each coefficient)\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n",
    "    mfccs_mean = np.mean(mfccs, axis=1)\n",
    "\n",
    "    # Spectral features\n",
    "    spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=audio, sr=sr))\n",
    "    spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=audio, sr=sr))\n",
    "    spectral_contrast = np.mean(librosa.feature.spectral_contrast(y=audio, sr=sr))\n",
    "\n",
    "    # Temporal feature\n",
    "    zero_crossing_rate = np.mean(librosa.feature.zero_crossing_rate(y=audio))\n",
    "\n",
    "    features = np.concatenate([mfccs_mean,\n",
    "                               [spectral_centroid, spectral_rolloff, spectral_contrast, zero_crossing_rate]])\n",
    "    return features\n",
    "\n",
    "\n",
    "def sliding_window_features(audio, sr, window_duration=1.0, hop_duration=0.5, n_mfcc=13):\n",
    "    \"\"\"\n",
    "    Segment the audio signal into overlapping windows and extract features for each window.\n",
    "    Returns a 2D array of shape (n_samples, n_features).\n",
    "    \"\"\"\n",
    "    window_length = int(window_duration * sr)\n",
    "    hop_length = int(hop_duration * sr)\n",
    "    features_list = []\n",
    "    for start in range(0, len(audio) - window_length, hop_length):\n",
    "        window = audio[start:start + window_length]\n",
    "        feat = extract_features(window, sr, n_mfcc=n_mfcc)\n",
    "        features_list.append(feat)\n",
    "    return np.array(features_list)\n",
    "\n",
    "\n",
    "# Paths for the two recordings (update these paths if needed)\n",
    "path_normal = \"../../Data/raw/13_real/Normal_knackgeräusche.wav\"\n",
    "path_anomaly = \"../../Data/raw/13_real/Anomaly_knackgeräusche.wav\"\n",
    "\n",
    "# Load recordings\n",
    "audio_normal, sr_normal = load_audio_file(path_normal)\n",
    "audio_anomaly, sr_anomaly = load_audio_file(path_anomaly)\n",
    "assert sr_normal == sr_anomaly, \"Sampling rates do not match!\"\n",
    "\n",
    "# Extract features from overlapping windows\n",
    "features_normal = sliding_window_features(audio_normal, sr_normal, window_duration=1.0, hop_duration=0.5)\n",
    "features_anomaly = sliding_window_features(audio_anomaly, sr_anomaly, window_duration=1.0, hop_duration=0.5)\n",
    "\n",
    "print(\"Normal features shape:\", features_normal.shape)\n",
    "print(\"Anomaly features shape:\", features_anomaly.shape)\n",
    "\n",
    "# Create labels: normal = 0, anomaly = 1\n",
    "labels_normal = np.zeros(features_normal.shape[0])\n",
    "labels_anomaly = np.ones(features_anomaly.shape[0])\n",
    "\n",
    "# Combine the data\n",
    "X = np.vstack([features_normal, features_anomaly])\n",
    "y = np.concatenate([labels_normal, labels_anomaly])\n"
   ],
   "id": "24408f2707ffdaf1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Data Preprocessing\n",
    "#\n",
    "We standardize the feature set and split the data into training and test sets.\n",
    "For the unsupervised models (K-means and autoencoder), we use only normal data for training.\n"
   ],
   "id": "6dc1b74da44a4d5e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into training and test sets (stratified by label)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# For unsupervised training on normal data:\n",
    "X_train_normal = X_train[y_train == 0]\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "print(\"Normal training set shape:\", X_train_normal.shape)\n"
   ],
   "id": "f748bb5dadadc184"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Model 1: K-means Clustering (Unsupervised)\n",
    "#\n",
    "We train K-means clustering on the normal data (assuming a single cluster).\n",
    "The anomaly score is computed as the minimum distance to the cluster center.\n",
    "A threshold is set (e.g., 95th percentile of training scores) to classify anomalies.\n"
   ],
   "id": "140d27dc03ec54e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Train K-means on normal training data\n",
    "n_clusters = 1  # Assume normal data is unimodal\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "kmeans.fit(X_train_normal)\n",
    "\n",
    "# Compute anomaly scores (distance to cluster center) for training and test sets\n",
    "train_scores = kmeans.transform(X_train_normal).min(axis=1)\n",
    "test_scores = kmeans.transform(X_test).min(axis=1)\n",
    "\n",
    "# Set threshold based on the 95th percentile of training anomaly scores\n",
    "threshold_kmeans = np.percentile(train_scores, 95)\n",
    "print(\"K-means threshold:\", threshold_kmeans)\n",
    "\n",
    "# Predict anomalies: if score > threshold, label as anomaly (1)\n",
    "y_pred_kmeans = (test_scores > threshold_kmeans).astype(int)\n",
    "\n",
    "# Evaluate K-means performance\n",
    "accuracy_kmeans = accuracy_score(y_test, y_pred_kmeans)\n",
    "precision_kmeans = precision_score(y_test, y_pred_kmeans)\n",
    "recall_kmeans = recall_score(y_test, y_pred_kmeans)\n",
    "f1_kmeans = f1_score(y_test, y_pred_kmeans)\n",
    "\n",
    "print(\"K-means Performance:\")\n",
    "print(\"Accuracy:\", accuracy_kmeans)\n",
    "print(\"Precision:\", precision_kmeans)\n",
    "print(\"Recall:\", recall_kmeans)\n",
    "print(\"F1 Score:\", f1_kmeans)\n"
   ],
   "id": "73d8e0db3be6128f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Model 2: Convolutional Autoencoder (Unsupervised)\n",
    "#\n",
    "We build a convolutional autoencoder that learns to reconstruct normal data.\n",
    "At test time, a high reconstruction error indicates an anomaly.\n",
    "#\n",
    "The input is reshaped to (samples, features, 1) to be compatible with 1D convolution layers.\n"
   ],
   "id": "3fe932926ae8482f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Reshape data for the autoencoder\n",
    "X_train_normal_ae = X_train_normal.reshape(-1, X_train_normal.shape[1], 1)\n",
    "X_test_ae = X_test.reshape(-1, X_test.shape[1], 1)\n",
    "input_shape = X_train_normal_ae.shape[1:]\n",
    "\n",
    "# Build a simple convolutional autoencoder\n",
    "input_layer = layers.Input(shape=input_shape)\n",
    "x = layers.Conv1D(32, 3, activation='relu', padding='same')(input_layer)\n",
    "x = layers.MaxPooling1D(2, padding='same')(x)\n",
    "x = layers.Conv1D(16, 3, activation='relu', padding='same')(x)\n",
    "encoded = layers.MaxPooling1D(2, padding='same')(x)\n",
    "\n",
    "x = layers.Conv1D(16, 3, activation='relu', padding='same')(encoded)\n",
    "x = layers.UpSampling1D(2)(x)\n",
    "x = layers.Conv1D(32, 3, activation='relu', padding='same')(x)\n",
    "x = layers.UpSampling1D(2)(x)\n",
    "decoded = layers.Conv1D(1, 3, activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = models.Model(input_layer, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.summary()\n",
    "\n",
    "# Train the autoencoder on normal data\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
    "history = autoencoder.fit(X_train_normal_ae, X_train_normal_ae,\n",
    "                          epochs=50, batch_size=32,\n",
    "                          validation_split=0.1, callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "# Compute reconstruction error on test set\n",
    "X_test_pred = autoencoder.predict(X_test_ae)\n",
    "reconstruction_error = np.mean(np.abs(X_test_pred - X_test_ae), axis=(1, 2))\n",
    "\n",
    "# Set threshold based on normal training error (95th percentile)\n",
    "X_train_pred = autoencoder.predict(X_train_normal_ae)\n",
    "train_error = np.mean(np.abs(X_train_pred - X_train_normal_ae), axis=(1, 2))\n",
    "threshold_ae = np.percentile(train_error, 95)\n",
    "print(\"Autoencoder threshold:\", threshold_ae)\n",
    "\n",
    "# Predict anomalies based on reconstruction error\n",
    "y_pred_ae = (reconstruction_error > threshold_ae).astype(int)\n",
    "\n",
    "# Evaluate autoencoder performance\n",
    "accuracy_ae = accuracy_score(y_test, y_pred_ae)\n",
    "precision_ae = precision_score(y_test, y_pred_ae)\n",
    "recall_ae = recall_score(y_test, y_pred_ae)\n",
    "f1_ae = f1_score(y_test, y_pred_ae)\n",
    "\n",
    "print(\"Autoencoder Performance:\")\n",
    "print(\"Accuracy:\", accuracy_ae)\n",
    "print(\"Precision:\", precision_ae)\n",
    "print(\"Recall:\", recall_ae)\n",
    "print(\"F1 Score:\", f1_ae)\n"
   ],
   "id": "400e72dcd244c50b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Model 3: Support Vector Machine (SVM) – Supervised\n",
    "#\n",
    "We train an SVM classifier on the standardized features.\n",
    "This supervised model uses labels to learn the distinction between normal and anomalous windows.\n"
   ],
   "id": "fca49e7ffccedb1c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Train the SVM classifier\n",
    "svm = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "\n",
    "# Evaluate SVM performance\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "precision_svm = precision_score(y_test, y_pred_svm)\n",
    "recall_svm = recall_score(y_test, y_pred_svm)\n",
    "f1_svm = f1_score(y_test, y_pred_svm)\n",
    "\n",
    "print(\"SVM Performance:\")\n",
    "print(\"Accuracy:\", accuracy_svm)\n",
    "print(\"Precision:\", precision_svm)\n",
    "print(\"Recall:\", recall_svm)\n",
    "print(\"F1 Score:\", f1_svm)\n"
   ],
   "id": "fb8c948efae1ee86"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. Model 4: Gaussian Mixture Model (GMM)\n",
    "#\n",
    "The GMM is trained on normal data only. For each sample, the log-likelihood is computed.\n",
    "Samples with log-likelihood below a threshold (set from the training distribution) are flagged as anomalies.\n"
   ],
   "id": "515a2d081457171b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Train a GMM on normal data\n",
    "gmm = GaussianMixture(n_components=1, covariance_type='full', random_state=42)\n",
    "gmm.fit(X_train_normal)\n",
    "\n",
    "# Compute log-likelihood scores for training and test data\n",
    "train_ll = gmm.score_samples(X_train_normal)  # Higher is better\n",
    "test_ll = gmm.score_samples(X_test)\n",
    "\n",
    "# Set threshold as the 5th percentile (i.e. very low likelihood indicates anomaly)\n",
    "threshold_gmm = np.percentile(train_ll, 5)\n",
    "print(\"GMM log-likelihood threshold:\", threshold_gmm)\n",
    "\n",
    "# Predict anomalies: if log-likelihood is below threshold, label as anomaly (1)\n",
    "y_pred_gmm = (test_ll < threshold_gmm).astype(int)\n",
    "\n",
    "# Evaluate GMM\n",
    "accuracy_gmm = accuracy_score(y_test, y_pred_gmm)\n",
    "precision_gmm = precision_score(y_test, y_pred_gmm)\n",
    "recall_gmm = recall_score(y_test, y_pred_gmm)\n",
    "f1_gmm = f1_score(y_test, y_pred_gmm)\n",
    "\n",
    "print(\"GMM Performance:\")\n",
    "print(\"Accuracy:\", accuracy_gmm)\n",
    "print(\"Precision:\", precision_gmm)\n",
    "print(\"Recall:\", recall_gmm)\n",
    "print(\"F1 Score:\", f1_gmm)\n",
    "\n"
   ],
   "id": "d38a80e354463852"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 8. Model 5: LSTM Autoencoder\n",
    "#\n",
    "Since an LSTM is designed for sequential data, we group consecutive sliding-window feature vectors into sequences.\n",
    "For example, with a sequence length of 10, each training sample is a 10-step sequence.\n",
    "#\n",
    "The LSTM autoencoder is trained on normal sequences. At test time, a high reconstruction error is an indicator of an anomaly.\n"
   ],
   "id": "6c36bd8f771ff44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_sequences(data, sequence_length=10):\n",
    "    \"\"\"Reshape 2D data (samples, features) into 3D sequences (n_sequences, sequence_length, features).\"\"\"\n",
    "    sequences = []\n",
    "    for i in range(len(data) - sequence_length + 1):\n",
    "        sequences.append(data[i:i + sequence_length])\n",
    "    return np.array(sequences)\n",
    "\n",
    "\n",
    "sequence_length = 10\n",
    "X_lstm_train = create_sequences(X_train_normal, sequence_length)\n",
    "X_lstm_test = create_sequences(X_test, sequence_length)\n",
    "\n",
    "print(\"LSTM training sequences shape:\", X_lstm_train.shape)\n",
    "print(\"LSTM test sequences shape:\", X_lstm_test.shape)\n",
    "\n",
    "# Build the LSTM Autoencoder\n",
    "timesteps = X_lstm_train.shape[1]\n",
    "feature_dim = X_lstm_train.shape[2]\n",
    "\n",
    "input_seq = layers.Input(shape=(timesteps, feature_dim))\n",
    "encoded = layers.LSTM(64, activation='relu', return_sequences=False)(input_seq)\n",
    "decoded = layers.RepeatVector(timesteps)(encoded)\n",
    "decoded = layers.LSTM(64, activation='relu', return_sequences=True)(decoded)\n",
    "decoded = layers.TimeDistributed(layers.Dense(feature_dim))(decoded)\n",
    "\n",
    "lstm_autoencoder = models.Model(input_seq, decoded)\n",
    "lstm_autoencoder.compile(optimizer='adam', loss='mse')\n",
    "lstm_autoencoder.summary()\n",
    "\n",
    "# Train the autoencoder on normal sequences\n",
    "early_stop = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
    "lstm_history = lstm_autoencoder.fit(X_lstm_train, X_lstm_train,\n",
    "                                    epochs=50,\n",
    "                                    batch_size=32,\n",
    "                                    validation_split=0.1,\n",
    "                                    callbacks=[early_stop],\n",
    "                                    verbose=1)\n",
    "\n",
    "# Compute reconstruction error on test sequences\n",
    "X_lstm_test_pred = lstm_autoencoder.predict(X_lstm_test)\n",
    "recon_error = np.mean(np.abs(X_lstm_test_pred - X_lstm_test), axis=(1, 2))\n",
    "\n",
    "# Set threshold from the training reconstruction errors\n",
    "X_lstm_train_pred = lstm_autoencoder.predict(X_lstm_train)\n",
    "train_recon_error = np.mean(np.abs(X_lstm_train_pred - X_lstm_train), axis=(1, 2))\n",
    "threshold_lstm = np.percentile(train_recon_error, 95)\n",
    "print(\"LSTM AE threshold:\", threshold_lstm)\n",
    "\n",
    "# For the test set, we need labels for each sequence.\n",
    "# For simplicity, assume that if any window in a sequence was anomalous, the sequence is anomalous.\n",
    "# Here we approximate by taking the mode from the overlapping segments.\n",
    "# (In practice, you might recompute sequences for the labeled data.)\n",
    "y_test_seq = y_test[sequence_length - 1:]  # This is a rough approximation\n",
    "\n",
    "# Predict anomalies from reconstruction error\n",
    "y_pred_lstm = (recon_error > threshold_lstm).astype(int)\n",
    "\n",
    "# Since lengths may not match exactly, evaluate on the available sequences\n",
    "accuracy_lstm = accuracy_score(y_test_seq, y_pred_lstm[:len(y_test_seq)])\n",
    "precision_lstm = precision_score(y_test_seq, y_pred_lstm[:len(y_test_seq)])\n",
    "recall_lstm = recall_score(y_test_seq, y_pred_lstm[:len(y_test_seq)])\n",
    "f1_lstm = f1_score(y_test_seq, y_pred_lstm[:len(y_test_seq)])\n",
    "\n",
    "print(\"LSTM AE Performance:\")\n",
    "print(\"Accuracy:\", accuracy_lstm)\n",
    "print(\"Precision:\", precision_lstm)\n",
    "print(\"Recall:\", recall_lstm)\n",
    "print(\"F1 Score:\", f1_lstm)\n"
   ],
   "id": "86ce16be04672245"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 9. Model 6: GAN-based Anomaly Detection\n",
    "#\n",
    "We build a simple GAN with fully-connected networks.\n",
    "The GAN is trained on normal features only. After training, the discriminator's output is used as\n",
    "an anomaly score (lower probability indicates anomaly).\n",
    "#\n",
    "Here we use a simple iterative training loop.\n"
   ],
   "id": "695c98c87d781ae5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# GAN parameters\n",
    "latent_dim = 10\n",
    "feature_dim = X_train_normal.shape[1]\n",
    "batch_size = 32\n",
    "epochs = 1000  # For demonstration; increase for better convergence\n",
    "\n",
    "\n",
    "# Build Generator: maps latent vector to feature space\n",
    "def build_generator():\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(32, activation='relu', input_dim=latent_dim),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(feature_dim, activation='linear')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Build Discriminator: maps feature vector to probability of being \"real\" (normal)\n",
    "def build_discriminator():\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_dim=feature_dim),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=0.0002), loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# Freeze discriminator when training the GAN (generator+discriminator)\n",
    "discriminator.trainable = False\n",
    "gan_input = layers.Input(shape=(latent_dim,))\n",
    "generated_features = generator(gan_input)\n",
    "gan_output = discriminator(generated_features)\n",
    "gan = models.Model(gan_input, gan_output)\n",
    "gan.compile(optimizer=optimizers.Adam(learning_rate=0.0002), loss='binary_crossentropy')\n",
    "\n",
    "# Prepare training data for GAN (normal features only)\n",
    "X_gan = X_train_normal\n",
    "real_labels = np.ones((X_gan.shape[0], 1))\n",
    "fake_labels = np.zeros((X_gan.shape[0], 1))\n",
    "\n",
    "# Training loop for GAN\n",
    "import tqdm\n",
    "\n",
    "n_batches = int(X_gan.shape[0] / batch_size)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for _ in range(n_batches):\n",
    "        # Train discriminator\n",
    "        idx = np.random.randint(0, X_gan.shape[0], batch_size)\n",
    "        real_data = X_gan[idx]\n",
    "\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        fake_data = generator.predict(noise)\n",
    "\n",
    "        d_loss_real = discriminator.train_on_batch(real_data, np.ones((batch_size, 1)))\n",
    "        d_loss_fake = discriminator.train_on_batch(fake_data, np.zeros((batch_size, 1)))\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # Train generator via GAN\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "\n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch {epoch} / {epochs} - D loss: {d_loss:.4f}, G loss: {g_loss:.4f}\")\n",
    "\n",
    "# After training, use the discriminator's output as an anomaly score.\n",
    "# For a given sample x, a low D(x) indicates that the sample does not resemble normal data.\n",
    "# Compute anomaly scores on the test set.\n",
    "d_scores = discriminator.predict(X_test)  # Probability of being normal\n",
    "# Define anomaly score as: score = 1 - D(x)\n",
    "anomaly_scores = 1 - d_scores.flatten()\n",
    "\n",
    "# Set threshold based on the 95th percentile of anomaly scores for normal training data.\n",
    "d_scores_train = discriminator.predict(X_train_normal)\n",
    "train_anomaly_scores = 1 - d_scores_train.flatten()\n",
    "threshold_gan = np.percentile(train_anomaly_scores, 95)\n",
    "print(\"GAN-based threshold:\", threshold_gan)\n",
    "\n",
    "y_pred_gan = (anomaly_scores > threshold_gan).astype(int)\n",
    "\n",
    "# Evaluate GAN-based anomaly detection\n",
    "accuracy_gan = accuracy_score(y_test, y_pred_gan)\n",
    "precision_gan = precision_score(y_test, y_pred_gan)\n",
    "recall_gan = recall_score(y_test, y_pred_gan)\n",
    "f1_gan = f1_score(y_test, y_pred_gan)\n",
    "\n",
    "print(\"GAN-based Model Performance:\")\n",
    "print(\"Accuracy:\", accuracy_gan)\n",
    "print(\"Precision:\", precision_gan)\n",
    "print(\"Recall:\", recall_gan)\n",
    "print(\"F1 Score:\", f1_gan)"
   ],
   "id": "282075d7d1461c8a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. Additional Evaluation: ROC Curves\n",
    "#\n",
    "We plot ROC curves for each model.\n",
    "- For K-means, we use the distance score (higher means more anomalous).\n",
    "- For the autoencoder, we use the reconstruction error.\n",
    "- For the SVM, we use the probability estimates for the positive class.\n"
   ],
   "id": "dc1380bdb74b986c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_roc(y_true, scores, model_name):\n",
    "    fpr, tpr, _ = roc_curve(y_true, scores)\n",
    "    auc = roc_auc_score(y_true, scores)\n",
    "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.2f})')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# K-means: use the distance scores (directly, higher is more anomalous)\n",
    "plot_roc(y_test, test_scores, \"K-means\")\n",
    "# Autoencoder: use reconstruction error\n",
    "plot_roc(y_test, reconstruction_error, \"Autoencoder\")\n",
    "# SVM: use predicted probabilities for the anomaly class\n",
    "svm_probs = svm.predict_proba(X_test)[:, 1]\n",
    "plot_roc(y_test, svm_probs, \"SVM\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curves\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "fc3d14811e180a74"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 8. Conclusions\n",
    "#\n",
    "- **K-means Clustering:** Uses a simple unsupervised approach based on the distance from the learned normal cluster.\n",
    "- **Convolutional Autoencoder:** Learns a compressed representation of normal data, with reconstruction error serving as an anomaly score.\n",
    "- **SVM:** Leverages labeled data in a supervised framework.\n",
    "#\n",
    "Evaluation metrics (accuracy, precision, recall, F1 score, ROC curves) provide insights into model performance.\n",
    "#\n",
    "Future work could include hyperparameter tuning, additional feature engineering, and exploring ensemble approaches to further improve detection performance.\n"
   ],
   "id": "6d9354966f6084b9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
